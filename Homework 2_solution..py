# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15rLbRgP7KMpvaqdnAjYun0kiiZa34b0R
"""

import numpy as np

# Confusion matrix: rows = predicted, columns = actual (Cat, Dog, Rabbit)
conf_matrix = np.array([
    [5, 10, 5],   # Predicted Cat
    [15, 20, 10], # Predicted Dog
    [0, 15, 10]   # Predicted Rabbit
])

classes = ['Cat', 'Dog', 'Rabbit']

# Initialize per-class precision and recall
precision = []
recall = []

# Compute per-class metrics
for i in range(len(classes)):
    tp = conf_matrix[i, i]
    fp = conf_matrix[i, :].sum() - tp
    fn = conf_matrix[:, i].sum() - tp
    prec = tp / (tp + fp) if (tp + fp) > 0 else 0
    rec = tp / (tp + fn) if (tp + fn) > 0 else 0
    precision.append(prec)
    recall.append(rec)
    print(f"{classes[i]} -> Precision: {prec:.3f}, Recall: {rec:.3f}")

# Macro-averaged metrics
macro_precision = np.mean(precision)
macro_recall = np.mean(recall)
print(f"\nMacro-Averaged Precision: {macro_precision:.3f}")
print(f"Macro-Averaged Recall: {macro_recall:.3f}")

# Micro-averaged metrics
tp_total = np.trace(conf_matrix)
fp_total = conf_matrix.sum(axis=1).sum() - tp_total
fn_total = conf_matrix.sum(axis=0).sum() - tp_total

micro_precision = tp_total / (tp_total + fp_total)
micro_recall = tp_total / (tp_total + fn_total)
print(f"\nMicro-Averaged Precision: {micro_precision:.3f}")
print(f"Micro-Averaged Recall: {micro_recall:.3f}")

from collections import defaultdict
import math

# 1. Training corpus
corpus = [
    "<s> I love NLP </s>",
    "<s> I love deep learning </s>",
    "<s> deep learning is fun </s>"
]

# 2. Tokenize corpus
tokenized_corpus = [sentence.split() for sentence in corpus]

# 3. Compute unigram counts
unigram_counts = defaultdict(int)
for sentence in tokenized_corpus:
    for word in sentence:
        unigram_counts[word] += 1

# 4. Compute bigram counts
bigram_counts = defaultdict(int)
for sentence in tokenized_corpus:
    for i in range(len(sentence)-1):
        bigram = (sentence[i], sentence[i+1])
        bigram_counts[bigram] += 1

# 5. Estimate bigram probabilities using MLE
def bigram_prob(w1, w2):
    return bigram_counts[(w1, w2)] / unigram_counts[w1]

# 6. Function to calculate probability of a sentence
def sentence_prob(sentence):
    words = sentence.split()
    prob = 1.0
    for i in range(len(words)-1):
        w1, w2 = words[i], words[i+1]
        bp = bigram_prob(w1, w2)
        prob *= bp
    return prob

# 7. Test sentences
sentences = [
    "<s> I love NLP </s>",
    "<s> I love deep learning </s>"
]

for sent in sentences:
    prob = sentence_prob(sent)
    print(f"Sentence: '{sent}' -> Probability: {prob:.6f}")

# 8. Determine which sentence the model prefers
probs = [sentence_prob(s) for s in sentences]
preferred_sentence = sentences[probs.index(max(probs))]
print(f"\nThe model prefers: '{preferred_sentence}' because it has higher probability.")